Capstone Topic: Researching the effectiveness of tokenization as method of data minimization for anomaly detection

Social and Cultural Concerns:
The overarching question in Machine Learning development is who will be replaced by an AI and who won’t. This research will aid those who want to replace as many Human resources in their pipeline as possible. Obviously research is a process but the capitalists who run the world cannot be expected to understand the process properly. It is possible that we see an era of poorly designed and implemented AI solutions because of the capitalists’ hatred towards spending money on human resources. 

Alot of developers may reject complete AI solutions to replace them, but what needs to get across is that we are still at least a decade away from that. What we have now are nothing more than complicated mathematical algorithms wrapped around some code. People need to understand that whatever threat they face from AI, they face that from the people who they work for, not the other way round.

Mitigation:
My plan for mitigating this issue is by raising awareness through the documentation of the project. I want to make it very clear that it is just an algorithm, the work done by it belongs to the one who ran it. Like it always has been. 

Citations:
Ford, M. (2013). Could artificial intelligence create an unemployment crisis?. Communications of the ACM, 56(7), 37-39.

Green, B., & Hu, L. (2018, July). The myth in the methodology: Towards a recontextualization of fairness in machine learning. In Proceedings of the machine learning: the debates workshop.
