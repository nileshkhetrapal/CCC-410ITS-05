{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web search results:\n",
    "\n",
    "[1] \"Define some parameters for the loader: batch_size = 32. img_height = 180. img_width = 180. It's good practice to use a validation split when developing your model. Use 80% of the images for training and 20% for validation. train_ds = tf.keras.utils.image_dataset_from_directory(.\"\n",
    "URL: https://www.tensorflow.org/tutorials/images/classification\n",
    "\n",
    "[2] \"Lets assume the following source and destination paths: dir_src = r\"/path/to/folder_with_subfolders\" dir_dst = r\"/path/to/destination_folder\" The directory \"folder_with_subfolders\" contains multiple sub-folders and their associated files as shown in the tree diagram below:\"\n",
    "URL: https://stackoverflow.com/questions/68881569/how-to-copy-images-from-multiple-sub-folders-to-a-common-folder-and-rename-them\n",
    "\n",
    "[3] \"Its a stacked value defined above as -. images = np.vstack (images) This same prediction is being appended into images_data. Assuming your prediction is not failing, it means every prediction is the prediction on all the images stacked in the images_data. So, for every iteration for i in range (len (images_data)): This images_data [i] [0] is ...\"\n",
    "URL: https://datascience.stackexchange.com/questions/81541/how-to-predict-multiple-images-from-folder-in-python\n",
    "\n",
    "\n",
    "Current date: 4/4/2023\n",
    "\n",
    "Instructions: Using the provided web search results, write a comprehensive reply to the given query. Make sure to cite results using [[number](URL)] notation after the reference. If the provided search results refer to multiple subjects with the same name, write separate answers for each subject.\n",
    "Query: I have a bunch of photos under different directories. The directories are classifications and the photos are training material. Create a python script that uses tensorflow keras to create a multiclass classifier that has layers for class map activation to have explainability into it's classification. Make sure to split the dataset into training and test and give the result of the model's accuracy at the end.\n",
    "\n",
    "Based on the information provided in the web search results, I will help you create a Python script using TensorFlow Keras for creating a multiclass classifier. This classifier will use class map activation for explainability, and the dataset will be split into training and testing datasets. The model's accuracy will be displayed at the end.\n",
    "\n",
    "First, let's import the necessary libraries and define the required parameters for the image loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "batch_size = 32 #Batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "img_height = 180\n",
    "img_width = 180\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a data preprocessing function that checks if the images are non-empty and have the correct dimensions, you can use the PIL library. If the images don't have the correct dimensions, this function will resize them to the specified height and width.\n",
    "\n",
    "First, install the Pillow library if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\nilekhet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (9.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a data preprocessing function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def preprocess_images(src_dir, dest_dir, img_height, img_width):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    for subdir, dirs, files in os.walk(src_dir):\n",
    "        for file in files:\n",
    "            img_path = os.path.join(subdir, file)\n",
    "            try:\n",
    "                # Open the image\n",
    "                img = Image.open(img_path)\n",
    "\n",
    "                # Check if the image is empty\n",
    "                if img.size == (0, 0):\n",
    "                    print(f\"Empty image {img_path}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Resize the image if necessary\n",
    "                if img.size != (img_width, img_height):\n",
    "                    img = img.resize((img_width, img_height))\n",
    "\n",
    "                # Save the preprocessed image to the destination directory\n",
    "                dest_img_path = os.path.join(dest_dir, os.path.relpath(img_path, src_dir))\n",
    "                os.makedirs(os.path.dirname(dest_img_path), exist_ok=True)\n",
    "                img.save(dest_img_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {e}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the preprocess_images function, provide the source directory with the original images, the destination directory where the preprocessed images will be saved, and the desired image height and width:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\devwork\\10dma\\10dma.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(dest_dir):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(dest_dir)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m preprocess_images(src_dir, dest_dir, img_height, img_width)\n",
      "\u001b[1;32md:\\devwork\\10dma\\10dma.ipynb Cell 8\u001b[0m in \u001b[0;36mpreprocess_images\u001b[1;34m(src_dir, dest_dir, img_height, img_width)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     dest_img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dest_dir, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mrelpath(img_path, src_dir))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X43sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(dest_img_path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X43sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     img\u001b[39m.\u001b[39;49msave(dest_img_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X43sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X43sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError processing image \u001b[39m\u001b[39m{\u001b[39;00mimg_path\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\Image.py:2300\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2297\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2299\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2300\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[0;32m   2301\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   2302\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\PngImagePlugin.py:1374\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1372\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode)\n\u001b[0;32m   1373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     ImageFile\u001b[39m.\u001b[39;49m_save(im, _idat(fp, chunk), [(\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)])\n\u001b[0;32m   1376\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[0;32m   1377\u001b[0m     \u001b[39mfor\u001b[39;00m info_chunk \u001b[39min\u001b[39;00m info\u001b[39m.\u001b[39mchunks:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\ImageFile.py:518\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m exc:\n\u001b[0;32m    516\u001b[0m     \u001b[39m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 518\u001b[0m         l, s, d \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode(bufsize)\n\u001b[0;32m    519\u001b[0m         fp\u001b[39m.\u001b[39mwrite(d)\n\u001b[0;32m    520\u001b[0m         \u001b[39mif\u001b[39;00m s:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "src_dir = \"D:\\\\devwork\\\\10dma\\\\extract.tar\\\\extract\"\n",
    "dest_dir = \"D:\\\\devwork\\\\10dma\\\\extract.tar\\\\extract2\"\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "import os\n",
    "#If the output directory does not exist, create it\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "preprocess_images(src_dir, dest_dir, img_height, img_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69958 images belonging to 307 classes.\n",
      "Found 17350 images belonging to 307 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image parameters\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "batch_size = 32\n",
    "\n",
    "# Create an ImageDataGenerator instance for preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize the pixel values to the [0, 1] range\n",
    "    # Add other data augmentation options if desired, such as rotation, zoom, etc.\n",
    ")\n",
    "\n",
    "# Create a data generator for the training dataset\n",
    "data_gen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "train_data = data_gen.flow_from_directory(\n",
    "    'D:\\\\devwork\\\\10dma\\\\extract.tar\\\\extract',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "val_data = data_gen.flow_from_directory(\n",
    "    'D:\\\\devwork\\\\10dma\\\\extract.tar\\\\extract',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the dataset and split it into training and validation sets. We'll use 80% of the images for training and 20% for validation as suggested in [1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87308 files belonging to 307 classes.\n",
      "Using 69847 files for training.\n",
      "Found 87308 files belonging to 307 classes.\n",
      "Using 17461 files for validation.\n"
     ]
    }
   ],
   "source": [
    "data_dir = dest_dir\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a multiclass classifier model with class map activation for explainability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_ds.class_names)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model and define the loss function, optimizer, and evaluation metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using the training dataset and validate it with the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nilekhet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 375/2183 [====>.........................] - ETA: 12:52 - loss: 2.5599 - accuracy: 0.4552"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nInput is empty.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_3277]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32md:\\devwork\\10dma\\10dma.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m#Epochs is the number of times the model will be trained on the entire dataset\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_ds,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_ds,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/devwork/10dma/10dma.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nInput is empty.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_3277]"
     ]
    }
   ],
   "source": [
    "epochs = 10 #Epochs is the number of times the model will be trained on the entire dataset\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's display the model's accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy = model.evaluate(val_ds)\n",
    "print(f\"Model accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions on an image with explanations, you can use the Grad-CAM (Gradient-weighted Class Activation Mapping) technique. Grad-CAM generates a heatmap highlighting the regions in the input image that contributed to the prediction. First, install the opencv-python library if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a function to make predictions with explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def load_image(path, target_size=(img_height, img_width)):\n",
    "    img = image_utils.load_img(path, target_size=target_size)\n",
    "    x = image_utils.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def grad_cam(model, img_array, layer_name):\n",
    "    # Get the output of the desired layer\n",
    "    output_layer = model.get_layer(layer_name).output\n",
    "\n",
    "    # Create a model with the same input as the original model, but with the output of the target layer\n",
    "    model_with_layer_output = Model(inputs=model.inputs, outputs=output_layer)\n",
    "\n",
    "    # Calculate the gradients of the target layer with respect to the model's output\n",
    "    with tf.GradientTape() as tape:\n",
    "        inputs = tf.cast(img_array, tf.float32)\n",
    "        tape.watch(inputs)\n",
    "        outputs = model_with_layer_output(inputs)\n",
    "        grads = tape.gradient(outputs, inputs)\n",
    "        \n",
    "    # Compute the guided gradients\n",
    "    guided_grads = tf.cast(outputs > 0, \"float32\") * tf.cast(grads > 0, \"float32\") * grads\n",
    "\n",
    "    # Calculate the weights for the guided gradients\n",
    "    weights = tf.reduce_mean(guided_grads, axis=(1, 2))\n",
    "\n",
    "    # Compute the Grad-CAM heatmap\n",
    "    cam = np.ones(outputs.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(weights[0]):\n",
    "        cam += w * outputs[0, :, :, i]\n",
    "    cam = cv2.resize(cam.numpy(), (img_width, img_height))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "\n",
    "    # Return the heatmap\n",
    "    return heatmap\n",
    "\n",
    "def predict_with_explanation(model, image_path, layer_name=\"conv2d_2\"):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_image(image_path, target_size=(img_height, img_width))\n",
    "\n",
    "    # Make the prediction\n",
    "    preds = model.predict(img_array)\n",
    "    pred_class = np.argmax(preds[0])\n",
    "\n",
    "    # Generate Grad-CAM heatmap\n",
    "    heatmap = grad_cam(model, img_array, layer_name)\n",
    "\n",
    "    # Load the original image\n",
    "    original_img = cv2.imread(image_path)\n",
    "    original_img = cv2.resize(original_img, (img_width, img_height))\n",
    "\n",
    "    # Superimpose the heatmap on the original image\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * 0.4 + original_img\n",
    "\n",
    "    # Display the original image, heatmap, and superimposed image\n",
    "    cv2.imshow(\"Original Image\", original_img)\n",
    "    cv2.imshow(\"Heatmap\", heatmap)\n",
    "    cv2.imshow(\"Superimposed Image\", superimposed_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Return the prediction class and its probability\n",
    "    return train_ds.class_names[pred_class], preds[0][pred_class]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the predict_with_explanation function to make predictions on an image with explanations. The function takes the trained model, the image path, and the name of the layer you want to use for generating the heatmap (by default, it uses the last convolutional layer in the example model).\n",
    "\n",
    "Here's an example of how to use the function with a test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"path/to/your/test_image.jpg\"\n",
    "predicted_class, probability = predict_with_explanation(model, image_path)\n",
    "print(f\"Predicted class: {predicted_class}, Probability: {probability:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict_with_explanation function will display the original image, the heatmap, and the superimposed image with the heatmap. It will also return the predicted class and its probability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the model using the save method and load it back using the load_model function from tensorflow.keras.models. Here's a function to save the model and another function to make predictions with explanations using the saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def save_trained_model(model, model_path):\n",
    "    model.save(model_path)\n",
    "\n",
    "def load_saved_model(model_path):\n",
    "    return load_model(model_path)\n",
    "\n",
    "def predict_with_saved_model(model_path, image_path, layer_name=\"conv2d_2\"):\n",
    "    # Load the saved model\n",
    "    saved_model = load_saved_model(model_path)\n",
    "\n",
    "    # Make a prediction with explanation using the saved model\n",
    "    predicted_class, probability = predict_with_explanation(saved_model, image_path, layer_name)\n",
    "\n",
    "    return predicted_class, probability\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save your trained model, use the save_trained_model function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"path/to/your/saved_model\"\n",
    "save_trained_model(model, model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can move the saved model to a different computer. To make predictions with explanations using the saved model, use the predict_with_saved_model function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"path/to/your/test_image.jpg\"\n",
    "predicted_class, probability = predict_with_saved_model(model_path, image_path)\n",
    "print(f\"Predicted class: {predicted_class}, Probability: {probability:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace model_path with the path to the saved model and image_path with the path to the image you want to make a prediction on. The predict_with_saved_model function will use the saved model to make predictions with explanations as described in the previous response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
